# Screenshot 2025-08-28 200039.png

For this exploration of emerging LLMs we divide models into:

Traditional LLMs — basic LLMs are trained on large datasets and using deep learning
techniques. LLM prompt engineering techniques, such as Chain-of-Thought (CoT), and
Retrieval-Augmented Generation (RAG) can be used to optimize traditional LLM
performance [Sharkey & Treleaven, 2025]:

Small Language Models (SLM) — SLM models are smaller in scale and scope, trained on
domain-specific data, and designed to be more computationally efficient, cost-effective,
and suitable for specific tasks or domains.

Large Language Models (LLM) — LLMs are Al models are trained on vast amounts of data
and context, and excels at processing, understanding, and generating human language.
data and use deep learning techniques, specifically transformer models, to learn
relationships.

Prompt Engineering (PE) - the process of designing and refining GenAl prompts which
are basically instructions, questions, or guidelines that guide LLM responses. An example
being Chain-of Thought reasoning.

Multi-model LLMs — composite models have been proposed where multiple specialized
models (experts) work together:

Large Deep Models — what we refer to as "deep" models use NLP combined with
reasoning capabilities and apply structured reasoning techniques:

Domain-specific LLMs — LLMs and Transformer models tailored for specific domains,
comprising: a) specific data types such as numeric or images; b) an industry sector, such
as finance, law, medicine, or education; and c) multiple domains:

Mechanistic LLMs — incorporating specific mechanism to optimize an LLM:

Underlying Network interpretability - understanding the internal reasoning processes of
trained neural networks. Several methods are being developed to improve the
interpretability of transformers, focusing on techniques like attention visualization,
attribution methods, and modifications to the model architecture itself:
