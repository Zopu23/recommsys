# Screenshot 2025-08-28 191813.png

Experiment 2: Advisor Module. The goal here is given a specific financial task or dataset the advisor will suggest the best-suited LLM (or approach) to use. For example, if the task is sentiment analysis on financial news,
the Advisor might recommend FinBERT as a strong candidate or if the task is broad Q&A on market data, it might suggest a larger model like BloombergGPT or FinGPT which are designed for a wide range of
financial tasks. A challenge we would need to address is that the Advisor needs knowledge of various models’ strengths and also Finance tasks vary widely (sentiment analysis, fraud detection, forecasting, Q&A on
reports, etc.), not to mention new models keep emerging. We need to find a way to optimise this.
Possible Techniques: we can use Knowledge Base & Rule-Based Mapping: so what we discussed with professor philip is that initially we can start focusing on 1 or 2 tasks eg sentiment analysis and we can create a
manual or rules-driven mapping from task requirements to recommended models. For example, if the task is “financial sentiment analysis,” we could say: “Use FinBERT.” This would be a very hands-on, expert-driven
approach. Basically, what we're building here is a kind of expert system. It works by encoding domain knowledge—like rules that say “For task X, model Y is known to work well.” Another example could be: “If you're
doing general financial forecasting, try FinGPT. or BloombergGPT.” As we go, we can keep expanding this knowledge base with more rules. Like:
“For fraud detection on transaction data, an LLM might not be necessary—use an anomaly detection model instead.”

“For summarizing long annual reports, go with a model that's fine-tuned for summarization.”

This kind of rule-based setup is really easy to implement—it doesn’t need fancy code. A simple if-else or a lookup table would be enough. But the cool part is that it's still based on expert insights, whether from
academic papers, online forums, or just what practitioners in the field are recommending. The big advantage here is transparency—users can actually see and understand why a certain model is being suggested. That
makes the system more trustworthy. However this might not be as scalable as other methods
Cons: It's static and manual. As new models appear or if tasks change, you must update the rules. It doesn’t scale well to “almost all” tasks in finance without becoming unwieldy. Also, a rule-based system might
oversimplify — it might miss nuances like “if dataset is very small, a smaller pre-trained model might outperform a large one without fine-tuning,” etc. Initially, however, this is a good bootstrap approach before more
automation.
We could have a Data-Driven Model Selection: A more scalable solution is to treat this as a model recommendation problem using data. Here we would maintain a database (possibly fed by the Radar module) of
models and their known performance on various benchmark tasks or datasets. For example, we could compile evaluation results from papers or leaderboards and store it in a knowledge base. Given a new user task
description, the system can retrieve the top LLM from this knowledge base. This could be as simple as filtering (e.g., find models that have the highest score on the relevant task) or it could be more complex such as
computing (um-bedings ) embeddings of the task description and finding similar known tasks. Another approach is a content-based recommender which would represent each model by features (for example
domains it's trained on, parameter size, speed) and each task by requirements (text length, needed accuracy, etc.), then use a similarity or machine learning model to match them.

The pros here is that this approach can adapt to new data automatically. If a new LLM comes out that outperforms others on, say, stock price forecasting, you update the database and the Advisor will start
recommending it. It can also consider multiple factors — e.g., if the user specifies a constraint like “must run on a laptop”, the system can prioritize smaller models in the database. Over time, this could become an
“AutoML” for model selection, picking the best model like how AutoML picks the best algorithm. However this requires gathering and updating a lot of information. We would have to rely on external evaluations
(which might not cover every niche task or the user's proprietary dataset). Implementation also is more complex: so we might need to build a search index or even train a secondary model that predicts performance.
Next we have LLM-based Advisory Agent which j think is the best and we should go with it: This technique will use a LLM itself to generate recommendations, effectively making the Advisor a conversational
agent. For instance, we could prompt an LLM (like GPT-4 or an open-source instruct model) with a description of the task and perhaps some context about available models, asking it to suggest the most suitable
model. We could also provide the Radar module's database summary as context. This method also has the potential to reason about the task and essentially, do the meta-thinking.

Pros: Flexible and natural — you can accommodate arbitrary user questions (“Which LLM is best for detecting fraud in transaction logs?") and let the Al consider various factors. It leverages the knowledge encoded in
the LLM (GPT-4, for example, knows about many models and techniques up to its training cutoff). This approach can also explain its recommendation in plain language, which is great for user trust. No separate
training is required if you use an existing powerful LLM; just craft effective prompts.
Cons: The advice is only as good as the LLM’s knowledge and the prompt. A general LLM might have outdated or incomplete info on very new models (unless you feed it via prompt). It can also hallucinate — e.g.,
recommend a model that doesn’t exist or exaggerate differences. You also have cost and dependency considerations: using an API like OpenAls is not free, and running a large model locally (if you choose an open
one) is computationally heavy. To mitigate this, you can constrain the LLM by providing facts (from the Radar database) and ask it to choose among those — this reduces the chance of it making stuff up. Another con
is that debugging or evaluating the correctness of its recommendation can be tricky.
