# RP3992.pdf

Machine Learning, Behavioral Targeting and

Regression Discontinuity Designs

Sridhar Narayanan

Kirthi Kalyanam

Stanford University

Santa Clara University∗

October 2021

Abstract

The availability of behavioral data on customers and advances in machine learn-
ing methods have enabled scoring and targeting of customers in a variety of domains,
including pricing, advertising, recommendation and personal selling. Typically, such
targeting involves ﬁrst training a machine learning algorithm on a training dataset, us-
ing that algorithm to score current or potential customers, and when the score crosses
a threshold, a treatment such as an oﬀer, an advertisement or a recommendation is as-
signed. In this paper, we highlight regression discontinuity designs (RDD) as a low-cost
alternative to obtaining causal estimates in settings where machine learning is used for
behavioral targeting. Our investigation leads to several new insights. Under appropri-
ate conditions, RDD recovers the local average treatment eﬀect (LATE). Further, we
show that RDD recovers the average treatment eﬀect (ATE) when: (1) The score is
orthogonal to the slope of the treatment and (2) When the selection threshold is equal
to the mean value of the score. We also show that RDD can estimate the bounds on the
ATE even if we are unable to get point estimates of the ATE. That RDD can estimate
ATE or bounds on ATE is a novel perspective that has been understudied in the liter-
ature. We also distinguish between two types of scoring: Intercept versus slope based
and highlight the practical value of RDD in each context. Finally, we apply RDD in an
empirical context where a machine learning based score was used to select consumers
for retargeted display advertising. We obtain LATE estimates of the impact of the
retargeted advertising program on both online and oﬄine purchases, and also estimate
bounds on the ATE. Our LATE estimates and ATE bounds add to the understanding

∗The authors can be reached at sridhar.narayanan@stanford.edu (Narayanan) and kkalyanam@gmail.com
(Kalyanam). They would like to thank participants of the 2019 and 2021 Marketing Science Conferences,
the 2020 AI/ML Conference and seminar participants at Washington University St. Louis and Santa Clara
University for their comments.

1

of the eﬀectiveness of retargeting programs in particular on oﬄine purchases which has
received less attention.

Keywords: Regression Discontinuity, Causal Eﬀects, Treatment Eﬀects, Machine Learn-
ing

1

Introduction

Behavioral targeting is one of the earliest and most popular marketing applications of ma-

chine learning. The recent explosion of highly granular ﬁrst party consumer data, have

allowed ﬁrms to utilize machine learning algorithms to score consumers and target them

based on these scores (Hitsch and Misra 2018). For example, an advertiser might observe

shopper browsing behavior and conversion rates on a website and then use a machine learn-

ing model to score conversion rates based on browsing behavior. Targeting rules can then be

set up on these scores - for example, consumers above a certain threshold on the score might

be targeted with promotions aimed at accelerating conversion. Such targeting policies have

been discussed and employed in a variety of contexts including recommendation systems

(Adomavicius and Tuzhilin 2005, Davidson et al. 2010, Gomez-Uribe and Hunt 2015, Smith

and Linden 2017), online advertising (He et al. 2014), customized pricing (Dube and Misra

2020) and in the context of personal selling (Syam and Sharma 2018).

In this paper, we highlight regression discontinuity designs (RDD) as a low-cost alterna-

tive to obtaining causal estimates in settings where machine learning is used for behavioral

targeting. We show that under appropriate conditions, RDD can provide local average treat-

ment eﬀects (LATE). However, machine learning contexts have an interesting feature in that

the score that they generate can be based on the slope of the treatment (slope score), the

purchase propensity (intercept score) or some combination of these . Incorporating the ideas

of slope versus intercept scoring into RDD yields several new insights/results: (1) When the

slope is orthogonal to the score, RDD yields the Average Treatment eﬀect (ATE), (2) When

the selection threshold, which is under the control of the ﬁrm, is equal to the mean value of

2

the score, RDD yields the ATE, (3) Even when point estimates of the ATE are not feasible,

RDD can generate bounds on the ATE.

RDD’s ability to obtain local average treatment eﬀects (LATE) has not only been a

primary focus of the literature but has also been the source of criticism. For example

Angrist and Pischke [2010] note the criticism of external validity and the related charge that

“experimentalists are playing small ball while big questions go unanswered”. Consequently

there has been some interest in going beyond LATE, and obtaining average treatment eﬀects

(ATE). Angrist and Rokkanen [2015] obtain causal estimates away from the cutoﬀ in a

regression discontinuity design using dependent variable predictors other than the running

variable or score. Conditional on these predictors the running variable is assumed to be

ignorable. Eckles et al. [2020] propose a new approach to identiﬁcation, estimation, and

inference in regression discontinuity designs that exploits exogenous measurement error in

the score variable. The approach proposed in this paper relies on the relationship between

the score and the slope of the treatment in a machine context to either obtain the ATE or

bounds on the ATE. Thus, we oﬀer a novel perspective about going from LATE to ATE that

is also very relevant to a variety of machine learning contexts.

The estimation of causal eﬀects of marketing treatments is of central interest to mar-

keters. The typical gold standard for obtaining causal eﬀects is an experiment, where ran-

domization of customers into treatment and control groups allows us to compare outcomes

for those treated and those who are not while keeping everything else ﬁxed between the two

groups. This addresses concerns that treated customers are systematically diﬀer from un-

treated customers due to reasons such as self-selection and endogeneity in treatment. This is

of particular concern in the behavioral targeting context, where customers are targeted with

treatment based on their past behavior. To the extent that past behavior is correlated with

the outcomes of interest, such treatment policies naturally lead to self-selection. Gordon

et al. [2019] show that estimates of advertising eﬀects using even state of the art methods on

observational data fails to obtain causal eﬀects - in magnitude and often even in terms of the

3

signs of the eﬀects. Gordon et al. [2021] discuss these issues as well. A more general discus-

sion of the challenges of inferring causality in machine learning contexts is provided by Judea

Pearl in various books including "The Book of Why" (Pearl and Mackenzie 2018). An alter-

native perspective to Gordon et al. [2021] is provided by Eckles and Bakshy [2017] who argue

that experiments might be ﬂawed in some situations and that having very high dimensional

data sets can produce estimates that are close to those obtained from experimentation.

A number of studies in the past have applied a variety of experimental designs to obtain

causal eﬀects of marketing interventions. One of the issues with experimentation is that it is

expensive and slow. Therefore, ﬁrms often experiment in an episodic rather than continuous

basis. This has led to calls for alternatives to experimentation (Eckles and Bakshy 2017),

that allow for causal measurement, but at low cost and on a continuous, rather than episodic

basis (Sharma et al. 2015, Gomez-Uribe and Hunt 2015). Researchers have used a variety

of quasi-experimental approaches that exploit naturally occurring randomness in the data

generating process to study the eﬀects of marketing treatments including search advertising

(Narayanan and Kalyanam 2015), television advertising (Liaukonyte et al. 2015, Hartmann

and Klapper 2018) and promotional oﬀers (Nair et al. 2011, 2017). These allow for causal

estimation but without the costs and time associated with experimentation.

The context of behavioral targeting leads to particular concerns around the use of quasi-

experimental approaches for causal estimation of treatment eﬀects. Nair et al. [2011] study

the contexts in which regression discontinuity designs can be used for ﬁnding causal esti-

mates of local average treatment eﬀects in contexts of behavioral targeting. The study looks

at situations where targeting is based directly on measures of past behaviors or summaries

of past behaviors, and makes the case that the researcher needs to carefully examine the

validity of RDD in these contexts. We build on this study by speciﬁcally examining the

validity and utility of RDD in contexts where targeting is based not on the variables summa-

rizing past behavior directly, but where a large number of such variables are used through

a machine learning framework to score customers. We document that this gives rise to a

4

natural application of machine learning algorithms for causal measurement. Importantly, we

document that under some conditions, RDD can be used to obtain not just local average

treatment eﬀects (LATE), but also average treatment eﬀects (ATE), a novel ﬁnding for this

literature. In some other contexts, it can be used to obtain bounds for the ATE even if point

estimates are not feasible. Thus, we propose an approach with practical utility for obtaining

treatment eﬀects of interest in a variety of contexts.

We show that machine learning based targeting policies present natural opportunities for

the use of regression discontinuity designs. This is because the large number of variables

underlying machine learning algorithms usually ensure that the score is continuous. A con-

tinuous score, combined with a threshold rule for treatment, where consumers above a given

threshold are treated and those below are not, meet the conditions for validity of regression

discontinuity designs (Hahn et al. 2001, Lee and Lemeiux 2010, Imbens and Lemieux 2008,

Nair et al. 2011). Thus, at the threshold at which there is treatment, we can obtain local

average treatment eﬀects using RDD. We additionally examine conditions under which we

can use RDD to go beyond LATE. With heterogeneous treatment eﬀects, we show that the

degree to which the consumer-level score is correlated with these treatment eﬀects matters.

When the score and the treatment eﬀects are uncorrelated, RD obtains ATE and not merely

LATE. Further, under some conditions, when the threshold for treatment is chosen care-

fully, the ﬁrm can obtain ATE even when the score and treatment eﬀects are correlated.

Finally, we derive the bounds for ATE as a function of the LATE estimates obtained using

the regression discontinuity design. With some prior knowledge regarding the variance of

the distribution of treatment eﬀects, or where there are naturally bounds on the variance,

we can obtain the bounds for the ATE.

We discuss the application of these results in two contexts in which RDD can be used

with behavioral targeting using machine learning based scoring systems. Machine learning

algorithms have been used in two ways to score customers.

In the ﬁrst, which we term

intercept-based scoring, customers are scored on their likelihood of having a positive out-

5

come. For instance, if the outcome is purchase, consumers might be scored on the likelihood

of making a purchase. If the outcome of interest is churn, then the likelihood of churning

constitutes the score. By contrast, consumers could be scored on their incrementality from

treatment - we term this slope-based scoring. The algorithm attempts to predict the incre-

mental eﬀects of the marketing treatment and bases the score on it. This is inherently harder

to do, as it requires the ﬁrm to have a way to measure this incrementality at the individual

customer level. We show that RDD provides a simple relatively low-cost way to assess the

validity of the slope-based scoring algorithm. In intercept-based scoring contexts, we ﬁnd

that the results on ATE discussed earlier can be of use under some conditions. Thus, we

discuss practical ways in which RDD can be employed in machine learning based targeting

contexts.

We then apply our approach to an empirical setting involving the retargeting of display

advertising. In this application, we partnered with a ﬁrm that used a machine learning score

to select customers for retargeted advertising. This score was obtained using a machine

learning algorithm that took as its input the consumers’ browsing and transaction activity.

Individuals whose scores exceeded a threshold were selected for the retargeting of display

advertising. We obtain estimates of the causal eﬀects of retargeted display advertising using

our regression discontinuity approach. We have data on both online and oﬄine purchases

and hence are able to investigate the cross channel impact of retargeted display advertising.

We also go beyond local average treatment eﬀects to obtain bounds on the average treatment

eﬀect.

In the next section, we discuss various applications of behavioral targeting and machine

learning. We then discuss regression discontinuity designs, speciﬁcally looking at conditions

under which they can be used to obtain local average treatment eﬀects, average treatment

eﬀects and bounds on the latter. We discuss applications of RDD to machine learning based

targeting contexts, speciﬁcally discussing intercept-based and slope-based scoring policies.

Next, we present our empirical application to retargeted display advertising. Finally, we

6

conclude with implications of our work, and limitations.

2 Behavioral Targeting and Machine Learning

2.1 Behavioral Targeting

Behavioral targeting has a long history in marketing. Some of the early documented exam-

ples of behavioral targeting are from catalog marketing (Shepard 1990). Catalog marketers

observe their customers’ response to catalog mailings. They then score customers based on

recency, frequency and monetary value of their response behaviors and select customers who

exceed a threshold for follow-up mailings or other marketing actions. The installation of

point of sale scanners in retail stores led to a dramatic increase in data on individual-level

purchase behaviors collected in the grocery channel (Blattberg 1988). This led to an increase

in the use of behavioral targeting in the grocery channel. For example, Catalina marketing,

a commercial coupon marketing company issues coupons at the checkout of grocery stores

based on observing a consumer’s checkout ticket. Some implementations of the Catalina

system were based on observing a single purchase of a shopper. Rossi et al. [1996] analyzed

the value of purchase data for behavioral targeting in a setting that emulated the Catalina

implementation. They examined the beneﬁts of a customization strategy where the face

value of a coupon is customized to an individual based on their purchase behavior.

2.2 Click Stream and Path to Purchase Data

The growth of the internet lead to a dramatic increase in the collection of behavioral data,

to an even greater extent than the arrival of scanners in stores. One of the most common

types of behavioral data available to ﬁrms is web site browsing data. Web servers create a

time-stamped log of each page accessed by a visitor. These logs, combined with a visitor’s

session id or session cookie can tie together the diﬀerent pages that were visited during a

session. In the context of a retail web site, these web logs can provide information on whether

7

a visitor added an item to a shopping cart, what items were added, how many pages were

browsed on a session and the dwell time on speciﬁc pages. By combining these data, the

visitor’s path through the web site can be traced. This type of behavioral data is often

referred to as click stream data and there is a growing literature on modeling these kinds

of data (Montgomery et al. 2004, Bucklin et al. 2002). The focus of these studies is to use

past behavior to predict future behavior. In addition to these onsite browsing behaviors,

the inbound traﬃc on a web site also has a reference to the website-the referring URL-that

was the source of the traﬃc. For example the referring URL would indicate if the visitor

came from an ad campaign on a search engine, or a price comparison engine or by directly

typing in the URL into the browser. Information on the inbound channel has also become

useful in building path to purchase models (Kannan et al. 2016). This combination of on

site browsing behavior and the referring channel creates a rich source of behavioral data for

diﬀerent types of targeting in many application areas in marketing.

The richness of the click stream and path to purchase data has created opportunities to

predict future behavior based on past behavior using machine learning models that typically

take as inputs a very large number of predictor variables, and aim to relate them to some

outcome of interest. Various methods have been developed to improve the predictive accuracy

(Breiman 2001) and the computational eﬃciency of these models (Sparapani et al. 2019).

The methodological developments in this ﬁeld are beyond the scope of this paper. But what

is of interest is that these machine learning applications focus on predicting future behavior

and do so by generating a score for each customer or visitor based on their past behavior.

Customers whose score crosses a certain threshold are then targeted for marketing actions.

The choice of the threshold can be driven by available budgets or by an economic criteria

such as return on investment or break-even analysis.

8

2.3 Online and Oﬄine Advertising

One of the most common application areas for behavior targeting is online or oﬄine ad-

vertising. For example, in a practice called retargeting, an advertiser can target display

advertising to visitors whose on site behaviors exceed a certain threshold in terms of the ma-

chine learning score. Advertisers can group customers into separate buckets and then score

them. Customers whose scores are above a cutoﬀ value receive advertising. For example

Sahni et al. [2019] demonstrate an example of the retargeting of display advertising to two

diﬀerent groups of customers - product viewers and cart creators. In the context of search

advertising, Google oﬀers a feature called Retargeting List for Search Advertising (Google

[2020]) that allows advertisers to score customers and retarget them for search advertising

campaigns. Online browsing behaviors can also be used to target oﬄine advertising.

In addition to advertisers targeting users, ad platforms such as Facebook can also target

advertising to users using behavioral data. He et al. [2014] describe the use of machine

learning to target ads based on predicted clicks at Facebook. On the Facebook platform, ads

are not associated with a query but instead with user demographic information, interests

and past behavior. Machine learning can be used to score the set of candidate ads and the

ads that are potentially above a threshold or score the highest can be selected to be shown

to the user. This is also a behavioral targeting context since the score is based on prior user

behavior.

2.4 Recommendation Systems in eCommerce, Content and Enter-

tainment Platforms

Machine learning models are also extensively used to build recommendation systems in elec-

tronic commerce contexts. These systems can be broadly classiﬁed into collaborative systems,

content systems and hybrid systems (Adomavicius and Tuzhilin 2005). Collaborative sys-

tems provide recommendations based on items that users with similar tastes and preferences

9

have liked in the past. Content based systems provide recommendations primarily based on

the content or products that the user has shown a preference for in the past. Hybrid systems

combine elements of both content based systems and collaborative systems.

The canonical example of collaborative recommendation systems is the system of recom-

mendations on Amazon. These systems as they started out were loosely deﬁned as "people

are likely to buy items that others like them bought". In other words the recommendation

systems generate a relatedness score based on past behavior of people with similar likes.

While these recommendation systems started out as a simple relatedness count system, they

now use machine learning to incorporate what recommendations are liked, clicked on, what

items are compatible, substitutes versus complements, sequential purchases and the impact

of time (Smith and Linden 2017). Reports suggest that a fairly large fraction of the clicks

received by a recommended product comes from a recommendation link (Smith and Linden

2017). However obtaining causal estimates of incremental clicks due to recommendation sys-

tems is challenging since they require experimentation that can inconvenience users (Sharma

et al. 2015). Natural experiments such as an those involving exogeneous shocks to traﬃc

have been proposed as an alternative (Sharma et al. 2015).

Another popular and highly visible example of collaborative recommendations are sys-

tems on video sites such as YouTube and Netﬂix. Users navigate the vast amounts of

information on content platforms such as YouTube using a combination of search, browsing

and recommendations. Recommendation systems on sites such as YouTube have many ob-

jectives. Some users have speciﬁc interests and some users have broad interests. Some users

want to be entertained. The scoring system on YouTube is a top N recommender (Davidson

et al. 2010). A well known technique to build recommendation systems is to use association

rule mining or co-visitation counts (Agrawal et al. 1993).

In addition to providing recommendations, the platform also observes many outcome

metrics such as whether the user accepted recommendations and other measures such as

whether the user increased the usage of the site. These outcomes metrics can be incorporated

10

into a machine learning model along with the relatedness score to generate a more composite

score that incorporates other factors such as past user behavior. Such a composite score could

generate a top N set of recommendations, where N is determined by the space available on

the display. The score could also be subject to a threshold value.

Collaborative recommendations are also extensively used in the online content streaming

industry. Netﬂix, the pioneer in this category initially started predicting user ratings for

video rentals but has now evolved to use an ensemble of prediction algorithms to help users

select content. According to Netﬂix, the typical consumer loses interest after 60 to 90

seconds, reviewing 10-20 titles across one or two screens (Gomez-Uribe and Hunt 2015). So

it is important to provide eﬀective recommendations. Netﬂix has multiple recommendations

categories such as ‘Top Picks’, ‘Trending Now’, ‘Continue Watching’, and ‘Because You

Watched’. Netﬂix also uses algorithms to construct pages and rows. These algorithms use

diﬀerent signals blending popularity and personalization to generate scores and rankings.

Netﬂix tracks many outcome metrics including the extent of catalog utilization by the user

and the take rate, the fraction of recommendations that resulted in a play.

2.5 Pricing and Personal Selling

Online behavior is also used to target price and promotional oﬀers. For example consider

the case of Ziprecruiter as discussed in Dube and Misra [2020]. The authors ﬁrst run an

experiment to understand the causal eﬀects of pricing. Customers in this experiment sign

up for a trial period and also provide background information. A high dimensional demand

model is then trained on this data. In a second experiment, the proposed pricing scheme,

i.e. customized oﬀers, are implemented out of sample. In the language of machine learning,

customers are scored regarding their price sensitivity using the high dimensional data that

was obtained in the ﬁrst experiment. This higher dimensional data includes usage behavior.

This type of behaviorally targeted pricing has a lot of appeal to ﬁrms that oﬀer content

subscriptions or are in the software as a service (SAAS) sector. Firms in this sector practice

11

a freemium model (Pujol 2010, Seufert 2013), where some basic features of a product are

oﬀered free with access to premium features for a fee. Behavioral targeting can be applied

to customize and target pricing for upgrades from the free version in freemium models.

Personal selling eﬀorts are perhaps the most costly part of the marketing mix. The

optimal allocation of sales eﬀort, territory design, balancing eﬀort across the product life

cycle and balancing the reliance on incentives has received considerable attention (Zoltners

et al. 2008). Behavior-based lead scoring has also been an important practice in the allocation

of sales eﬀort. For example in the pharmaceutical industry, doctors are scored into deciles

based on their past prescription behavior and sales eﬀort is then assigned diﬀerentially for the

deciles (Narayanan and Manchanda 2009). In the situation where leads are generated online,

the behavior of the prospect or customer on the web site can be used to score the quality

of the lead before passing them on for follow up by the sales force. Marketing automation

platforms such as Salesforce.com and Marketo have started providing guidelines for scoring

diﬀerent behaviors of leads. For example some of the online behaviors include the landing

pages visited, emails opened, data or spec sheets downloaded, product and service pages

visited, the use of a part ﬁnder, the use of inventory lookup, visiting urgent delivery segments

of the web site.1. The scores are assigned by the marketing function within the organization

to generate a marketing qualiﬁed lead (MQL)2 and by the sales function to generate a sales

qualiﬁed lead (SQL)3. The leads can be scored on diﬀerent criteria by assigning points to

diﬀerent behaviors or using an automated algorithm. Lead scores that cross a threshold are

typically assigned for follow up by the sales team.

2.6 Summary and Evaluation of Behavioral Targeting

As the discussion above shows, behavioral targeting has been extensively applied across the

entire marketing mix. A common theme across all of these applications is that there is a

1https://www.salesforce.com/products/marketing-cloud/best-practices/basic-science-behind-lead-

scoring/

2https://blog.hubspot.com/marketing/deﬁnition-marketing-qualiﬁed-lead-mql-under-100-sr
3https://blog.marketo.com/2018/07/is-your-lead-sales-qualiﬁed-how-to-tell.html

12

treatment (a catalog mailing, a customized coupon, an oﬄine or an online ad, a recommen-

dation, a personalized price, a sales call). Individuals or treatments (a personalized price,

a recommendation) are scored by a machine learning algorithm using prior behavior data.

The individual or treatment is assigned based on the score crossing a threshold or a rank

ordering based on the score. Outcomes are observed both for customers with and without

treatment. For instance, in the case of targeting a speciﬁc ad for display advertising, the

outcomes would be subsequent visits to the website of the advertiser or purchasing the ad-

vertiser’s products, and these would be tracked for customers who were targeted with ads as

well as those who were not.4

The common key evaluative question is the causal eﬀect (incrementality) of behavioral

targeting using machine learning. For example if the conversion rate (or other outcomes)

for the customer who was selected using the machine learning score and was shown an ad is

statistically diﬀerence from the customer who was not selected for the ad using the machine

learning score, and the treatment was experimentally assigned using randomization, then the

measured eﬀects of the ad would be considered causal. However, in spite of the widespread

use of machine learning in behavioral targeting, evaluation is an ongoing challenge. Eval-

uation approaches include prediction testing of scoring models on holdout samples. There

is some awareness and recognition that the results from these models have to be carefully

evaluated for incrementality. For example in the context of recommendation engines, the

volume of clicks from the recommendation engine are often viewed as a measure of its suc-

cess. Using a data set in the context of recommendations on Amazon, Sharma et al. [2015]

point out that although recommendation clicks account for a large fraction of the traﬃc for

recommended products, at least 75% of this activity would likely occur in the absence of

recommendations (i.e. they are not incremental). They propose an instrumental variable

strategy that relies on demand shocks to estimate the incrementality of behavioral targeting

of recommendations. We note that ﬁnding credible instrumental variables is likely to be

4Advertising platforms such as Google and Facebook can track these “conversion” events on the retailer

platforms using what is called a conversion pixel.

13

context speciﬁc and in general challenging (Angrist and Pischke 2010).

A/B testing and ﬁeld experimentation have also been proposed in the context of behav-

ioral targeting using machine learning. For example, researchers at Netﬂix report using A/B

tests to evaluate recommendation algorithms (Gomez-Uribe and Hunt 2015). However a

recurring theme is that ﬁeld experiments are costly, that they may disadvantage some users,

and that they inconvenience users (Sharma et al. 2015) and are a bottle neck for rapid in-

novation. In their evaluation of recommendation systems at Netﬂix, Gomez-Uribe and Hunt

[2015] also state that behavior-based recommendation systems are subject to strong feedback

loops and hence require careful validation. Narayanan and Kalyanam [2015] also note the

existence of strong feedback loops but in the context of search advertising. To continue with

the Netﬂix example, Gomez-Uribe and Hunt [2015] voice concerns that experimentation is a

bottleneck for rapid innovation and explore oﬄine experimentation (backcasting models and

other forms of holdout prediction) as an alternative. However they conclude that

“We need to have a better alternative to oﬄine experimentation that allows

us to iterate as quickly, but that is more predictive of A/B test outcomes.”

A similar perspective is echoed by Adomavicius and Tuzhilin [2005] in their assessment

of recommendation systems. They note that

“Understandably, it is expensive and time consuming to conduct controlled

experiments with users ..therefore experiments that test recommendation quality

on an unbiased random sample are rare”

These perspectives make the case for the development of methods that provide more

frequent estimates, are causal but require less eﬀort and costs than a ﬁeld experiment. Our

perspective is that RDD-based methods meet this criteria of causal, more frequent and less

costly (in terms of time and money) estimates. But an important question is whether RDD

can indeed be an alternative to A/B test outcomes or ﬁeld experiments that typically yield

an average treatment eﬀect (ATE) estimate. We examine this important issue in this paper.

14

2.7 Types of Scoring - Intercept-scoring and Slope-scoring

Machine-learning based scoring algorithms can be broadly classiﬁed into two buckets -

intercept-based scoring and slope-based scoring.

In intercept-based scoring, as the name

suggests, the ﬁrm is attempting to estimate heterogeneous intercepts across customers and

target them based on this score crossing a threshold. Examples include ﬁnding the likelihood

of churn for a subscription product in order to target consumers with proactive retention

programs (Ascarza et al. 2018). Such types of scoring is quite common, as it relies on simpler

algorithms that relate outcomes of interest to a set of observable variables for consumers.

Slope-based scoring, on the other hand, attempts to target customers based on an es-

timate of how they are likely to respond to the marketing intervention itself (Athey et al.

2017, Hitsch and Misra 2018, Wager and Athey 2018. These are harder to implement as

they typically require estimation of heterogeneous treatment eﬀects, using experiments to

exogenously determine the treatment, and algorithms to obtain conditional average treat-

ment eﬀects (CATE). These CATEs in turn can be used for generating optimal targeting

policies (Hitsch and Misra 2018). This is less commonly applied in practice because of the

requirements it places on the data, the relative recency of the algorithmic approaches to this

problem and the technical sophistication required on the part of ﬁrms to implement such

policies. In this paper, we will discuss the estimation of causal eﬀects using RDD in both of

these situations, where consumers are scored on their intercepts and when they are scored

on their slopes.

15

3 Regression Discontinuity and Behavioral Targeting with

Machine Learning

3.1 Background on Regression Discontinuity

Regression discontinuity designs (RDD) can be employed to measure treatment eﬀects when

treatment is based on whether an underlying continuous forcing variable or score crosses a

threshold. Under the condition that there is no other source of discontinuity, the treatment,

which applies only to the observations with score above the threshold, induces a discontinuity

in the outcome of interest at the threshold. Thus, the limiting values of the outcome on the

two sides of the threshold are unequal and the diﬀerence between these two directional limits

measures the treatment eﬀect. A necessary condition for the validity of the RD design is

that the forcing variable itself is continuous at the threshold (Hahn et al. 2001).

Formally, let i index the observation, let yi denote the outcome of interest, xi the treat-

ment and zi the forcing variable (henceforth referred to as score), with ˜z being the threshold

above which there is treatment. Thus, treatment is deﬁned by

xi = 1 ⇐⇒ zi ≥ ˜z

xi = 0 ⇐⇒ zi < ˜z

Then the RD estimate of the treatment eﬀect β is given by

ˆβRD = lim

λ→0

E [yi|zi = ˜z + λ] − lim
λ→0

E [yi|zi = ˜z − λ] , λ > 0

(1)

(2)

(3)

Practical implementation involves ﬁnding these limiting values non-parametrically using

a local regression, often simply a local linear regression (Fan and Gijbels 1996) within a pre-

speciﬁed bandwidth λ of the threshold ˜z and then assessing sensitivity to the bandwidth.

16

More details on estimating causal eﬀects using RD designs, including the diﬀerence between

sharp and fuzzy RD designs, the selection of non-parametric estimators for the limits on the

two sides, the choice of bandwidth λ and the computation of standard errors can be found

in Hahn et al. [2001], Imbens and Lemieux [2008] and Lee and Lemeiux [2010].

3.2 Behavioral Targeting and Regression Discontinuity

While the basic scoring and selection aspects of machine learning seem to ﬁt the requirements

of a RDD, the behavioral targeting raises particular concerns about validity of RDD. The

fact that past behaviors are used to target customers with marketing interventions raises

questions about self-selection by consumers who might undertake some actions with antici-

pation of future marketing interventions that might beneﬁt them. For instance, consider a

loyalty program that has beneﬁts for consumers with score crossing a particular threshold.

Consumers who are aware of the policy and their own scores might be induced to undertake

actions that makes their score z cross the threshold ˜z, and thereby receive the beneﬁts. This

would make RD invalid, because the customers who chose to undertake actions to cross

the threshold would not be otherwise comparable with customers who did not, even at the

limit. Nair et al. [2011] discuss the conditions under which RDD can obtain causal eﬀects

in such contexts, pointing out that uncertainty about the score or threshold might allow for

measurement of local average treatment eﬀects (LATE) using RDD.

In this paper, we consider contexts where the behavioral targeting policy uses an un-

derlying machine learning algorithm to generate scores. The prototypical machine learning

context involves a large number of variables - that is often the reason a machine learning

algorithm is needed in the ﬁrst place. Most of these variables are based on behavioral data of

consumers - for instance, the history of browsing activity or purchases. These are combined

into a score through a complex algorithm. The very complexity of the algorithm, the fact

that it is based on a large number of behavioral variables and that the scores and thresholds

for treatment are unobserved support the validity of RDD in these contexts. The continuous

17

nature of the score, the fact that consumers are unable to anticipate how speciﬁc actions

they take aﬀect the score, and uncertainty of the score and threshold satisfy the conditions

laid out in Nair et al. [2011] for validity of RDD. This makes RDD a useful candidate to

measure LATE.

In marketing contexts these local eﬀects are often interesting in and of themselves, since

they measure the eﬀect of treatment at a relevant margin - the threshold at which treatment

takes place. The threshold is typically selected based on some underlying objective. The

objective might be to target customers who have positive expected proﬁts and measuring the

treatment eﬀect for the marginal customers who have expected proﬁts at the zero threshold

is often critically important for ﬁrms. However, randomized controlled experiments allow

the marketer to obtain average treatment eﬀects. These help evaluate the policy as a whole.

For instance, average treatment eﬀects help a marketer evaluate whether there are positive

returns on average to advertising. RDD typically fails to address this need to obtain average

treatment eﬀects. In the next section, we examine whether and under what conditions RDD

can go beyond LATE and obtain average treatment eﬀects.

3.3 Beyond local average treatment eﬀects

For the purpose of this analysis, consider the following data generating process.

yi = αi + βi · xi + εi

(4)

In this data generating process, the intercept αi, slope or treatment eﬀect βi and idiosyn-

cratic shock εi are allowed to be heterogeneous. This linear speciﬁcation allows us to analyze

the estimation of treatment eﬀects using RDD in a tractable manner. Given that the RDD

relies on ﬁnding limiting values of the outcome on the two sides of the threshold, the linear

speciﬁcation is a reasonable one, as any continuous and diﬀerentiable function can be locally

approximated by a linear speciﬁcation. The conditions of continuity and diﬀerentiability are

18

typical requirements of the RDD itself, and therefore, these are not additional conditions we

are imposing.

The treatment eﬀect is βi but since we do not observe the same unit of observation in

both treated and untreated (i.e. control) conditions, the inferential aim is average treatment

eﬀects. RDD speciﬁcally measures the local average treatment eﬀects in equation (3) where

we substitute the expression for yi in the data generating process in equation (4) to obtain

ˆβRD = lim

λ→0

E [αi + βi · xi + εi|zi = ˜z + λ] − lim
λ→0

E [αi + βi · xi + εi|zi = ˜z − λ]

(5)

Separating terms in these expectations and substituting xi as 0 or 1 based on the treatment

conditions in equations (1) and (2), we get

ˆβRD = lim

λ→0

E [αi|zi = ˜z + λ] − lim
λ→0

E [αi|zi = ˜z − λ] +

lim
λ→0

E [βi · 1|zi = ˜z + λ] − lim
λ→0

E [βi · 0|zi = ˜z − λ] +

(6)

lim
λ→0

E [εi|zi = ˜z + λ] − lim
λ→0

E [εi|zi = ˜z − λ]

In this equation, the continuity of the score zi, the intercept αi and the shock εi implies that

the limits of the expectations on the two sides of the threshold are equal. Thus,

lim
λ→0

E [αi|zi = ˜z + λ] = lim
λ→0

E [αi|zi = ˜z − λ]

lim
λ→0

E [εi|zi = ˜z + λ] − lim
λ→0

E [εi|zi = ˜z − λ]

and

Thus, we obtain

ˆβRD = lim

λ→0

E [βi|zi = ˜z + λ]

19

(7)

(8)

(9)

Although these are not new results to the literature, there are some important points to

reiterate based on the above formalization. First, the regression discontinuity estimate is the

local average treatment eﬀect at the treatment threshold, i.e. it is the local average treatment

eﬀect for those observations for which the score is the threshold. Second, this estimate is not

aﬀected by how the intercept αi and idiosyncratic shock εi are correlated with the score, as

long as the condition of continuity of everything other than the treatment at the threshold

is maintained. But the local average treatment eﬀect does rely on the correlation between

the heterogeneous treatment eﬀect or slope βi and the score zi. We will explore this next.

3.3.1 Score is Orthogonal to Slope

Let’s take the case when the score is orthogonal to the slope. Thus, zi ⊥ βi. Under this

condition, the expectation in equation (9) becomes an unconditional expectation. And since

the expectation is unconditional, the limit is the same at any value of zi. Thus,

zi ⊥ βi =⇒ ˆβRD = E [βi]

(10)

This expected value of βi is the average treatment eﬀect (ATE). Thus, we can see that

when the score is uncorrelated with the slope, we can obtain the ATE using a regression

discontinuity design. While this result may seem obvious, we note that to the best of our

knowledge, this result has not been highlighted or discussed in the literature.

3.3.2 Score and Slope are Correlated, and ˜z = ¯z

We next explore what happens when the score and slope are correlated. To do this, consider

the relationship between the score and the slope to be given by the following relationship

βi = γ0 + γ1zi + ηi

(11)

Consider the situation where the score and the shock in this expression are uncorrelated,

20

i.e. zi ⊥ ηi. This may in general be diﬃcult to justify, but in contexts where there is a large

volume of data, typical to machine learning based targeting situations, this may not be an

unreasonable assumption.

Then, substituting equation (11) into the LATE expression in equation (9), we get

ˆβRD = lim

λ→0

E [γ0 + γ1zi + ηi|zi = ˜z + λ]

(12)

= lim
λ→0

E [γ0|zi = ˜z + λ] + lim
λ→0

E [γ1zi|zi = ˜z + λ] + lim
λ→0

E [ηi|zi = ˜z + λ]

With a zero mean, uncorrelated error ηi, the third term in the expression goes to 0. We

therefore get

ˆβRD = γ0 + γ1 lim

λ→0

E [zi|zi = ˜z + λ]

= γ0 + γ1z

(13)

The ATE is the unconditional expectation of βi, which when substituting in the expression

of βi in equation (11) is

E [βi] = E [γ0 + γ1zi + ηi] = γ0 + γ1 ¯z

(14)

where ¯z = E [zi] is the mean value of the score. Therefore, the diﬀerence between the LATE

and ATE is given by

ˆβRD − E [βi] = γ1 (˜z − ¯z)

(15)

Equation (15) provides us with another important result. When the threshold for treat-

ment ˜z is the same as the mean of the score ¯z, the diﬀerence between the LATE and ATE

is zero. In other words when ˜z = ¯z, the RD estimate is equal to the ATE. Note that we are

assuming that the slope and score are correlated as per Equation (11) and that this result

does not require any additional assumptions. This is an important result because the ﬁrm

21

has the threshold ˜z under its control in many marketing contexts. The ﬁrm chooses this

threshold, and by setting it to be the mean score in even a subset of the observations or for

a subset of time periods, it can ﬁnd average treatment eﬀects without having to experiment

with the entire set of users.

Thus, RDD provides opportunities for continuous, and relatively low-cost estimation of

average treatment eﬀects. Firms are often concerned about the opportunity costs involved

in setting aside a group of consumers in a control group who do not receive the treatment

concerned and the costs involved in targeting a random set of users in the treatment group,

even though some of them might be poor targets. Adjusting an already established threshold-

based targeting policy might be easier to implement and be seen as less costly or risky. And

such measurement using the RDD approach provides a continuous evaluation of the average

treatment eﬀects rather than the episodic evaluation feasible using an experimental approach.

We now turn our attention to obtaining bounds for the ATE.

3.3.3 Bounds for the Average Treatment Eﬀect

Given that γ1 is the slope coeﬃcient in the regression of zi on βi and substituting in the

expression for γ1 as the ratio of the covariance of βi and zi and the variance of zi

γ1 =

cov (βi, zi)
var(zi)

we get

ˆβRD − E [βi] =

cov (βi, zi)
var(zi)

(˜z − ¯z)

= correlation (βi, zi)

(cid:115)

var (βi)
var(zi)

(˜z − ¯z)

(16)

(17)

where we have used the relationship between covariances, correlations and variances.

Now, let us consider the case when the correlation between the slope and score is 1. In

22

this situation

ˆβRD − E [βi] =

(cid:115)

var (βi)
var(zi)

(˜z − ¯z)

Similarly, when the correlation is -1, we get

(cid:115)

ˆβRD − E [βi] = −

var (βi)
var(zi)

(˜z − ¯z)

(18)

(19)

For values of this correlation between 1 and -1, it is easy to see that this diﬀerence

between the LATE and ATE also has to lie between these two extremes. Thus, we get the

result

(cid:32)

(cid:115)

E [βi] ⊂

ˆβRD −

var (βi)
var(zi)

|˜z − ¯z|, ˆβRD +

(cid:115)

var (βi)
var(zi)

(cid:33)

|˜z − ¯z|

(20)

The absolute value for the diﬀerence (˜z − ¯z) comes from the fact that which of the two bounds

is greater than the other depends on whether the threshold ˜z is greater or lesser than the

mean score ¯z.

Note that in the expression above, the bounds for the average treatment eﬀect E [βi]

depend on the threshold, the mean score and RD estimate, which are all known. The only

unknown is the variance of the treatment eﬀect. In some situations, the researcher might

have prior knowledge about this variance, or might be able to bound it (for instance, when

the dependent variable is bounded, like in the case of a binary dependent variable).

3.4 Discussion of the Utility of RDD in Intercept-based scoring

Now we discuss the utility of RDD approaches to ﬁnding treatment eﬀects in cases where the

scoring approach involves estimating the intercepts for customers, and setting thresholds on

these scores to determine which customers to target. In other words, the score is an estimate

of the intercept αi in equation (4).

zi = ˆαi

(21)

We have seen that RDD can be used to obtain local average treatment eﬀects under

23

conditions of continuity of the slope at the threshold. This, in and of itself, is interesting

in a variety of contexts. An understanding of eﬀects of marketing treatments at the margin

at which such treatments are undertaken can provide interesting insights to the marketer,

as seen in the examples in Nair et al. [2011]. Estimates of treatment eﬀects at the margins

can also in some instances provide marketers an assessment of how far away from optimal

their targeting policies are. For instance, the objective of a retargeting policy might be to

generate positive expected proﬁts. If the marginal customer has expected proﬁts far away

from zero, it might provide the marketer a signal about the departure from optimality of

such a policy. Experimenting with the targeting threshold, and continuous evaluation of the

local average treatment eﬀects at the various experimental thresholds might provide ﬁrms

with assessments of their targeting policies while retaining the simplicity of intercept-based

scoring.

We have also seen that we can use RDD to obtain average treatment eﬀects - ATE - in

the special case where the score and the slope are orthogonal. While this is a theoretical

argument, it may still be of relevance in contexts where it can be argued that the intercept

(which is what the score is trying to estimate) and the slope are uncorrelated.

More practical than this is the result we have found earlier for bounds on the estimate

of the ATE as a function of the RDD-based estimate. Thus, with either prior knowledge of

the variance of the treatment eﬀect var (βi) or with bounds placed on this, we can estimate

bounds on the ATE. Further, if the ﬁrm sets the threshold ˜z = ¯z, then RDD provides the

ATE.

3.5 Discussion of the Utility of RDD in Slope-based scoring

In slope-based scoring, the ﬁrm attempts to estimate the treatment eﬀect of interest for

each customer, and uses this estimate as a score in a targeting policy. For instance, an

advertising targeting policy might attempt to score customers on estimates of their response

to the advertising campaign, estimate the return on investment for each customer, and then

24

target customers with positive returns. Thus, the score is

zi = ˆβi

Substituting this in the expression for the RDD estimate in equation (9), we get

ˆβRD = lim

λ→0

E

(cid:105)
(cid:104)
βi| ˆβi = ˜z + λ

(22)

(23)

If ˆβi is a consistent estimate of the treatment eﬀect βi, it is true (at least aymptotically),

that

ˆβRD = ˜z

(24)

This is an important result because under the conditions we have assumed (consistency

of the estimator, continuity of the score), we see that the regression discontinuity estimate

of the local average treatment eﬀect has to equal the threshold for treatment itself. If it

does not, the assumptions underlying this result have to be untrue. With continuity of ˆβi

typically not in question in machine learning applications, it would imply that the estimate

is not consistent.

Thus, in slope-based scoring contexts, RDD provides a way to continuously evaluate the

validity of the underlying machine learning algorithm itself. If the underlying algorithm is

correctly estimating the treatment eﬀects, the RDD-based LATE estimate must be equal

to the threshold itself. If not, it would call for re-evaluation of the underlying algorithm or

suggest that the algorithm is not consistent with slope based scoring. This is an important

result as it would allow an analyst to discern the nature of the scoring rule and validate

institutional perspectives in this regard.

This provides an alternative to the experimental evaluation of the algorithm, which can

be costly and time-consuming and episodic. By contrast, with RDD we can obtain an

evaluation that is continuous. This is of particular relevance since even an experimental

25

validated algorithm for estimating slope might need reevaluations as time passes, market

conditions change, technologies change and new competitors enter the market. Continuous,

low-cost evaluation of the algorithm can be of potentially great value to marketers employing

slope-based scoring for behavioral targeting.

3.5.1 Summary of Key Theoretical Results

A summary of our key theoretical results (also summarized in Table 1) are as follows:

1. In intercept based scoring, when zi ⊥ βi =⇒ ˆβRD = E [βi] which is the ATE.

2. In Intercept based scoring, assuming a heterogeneous data generation process as given

by Equation (15), then ˜z = ¯z, =⇒ ˆβRD = E [βi], which is the ATE.

3. In intercept based scoring, the bounds for the ATE E [βi] are given by

(cid:32)

(cid:115)

ˆβRD −

var (βi)
var(zi)

|˜z − ¯z|, ˆβRD +

(cid:115)

var (βi)
var(zi)

(cid:33)

|˜z − ¯z|

(25)

.

In all of the above three cases since the decision maker does not have a slope based

score, the ATE or the bounds on the ATE obtained via RDD should be useful to the

decision maker.

4. In slope based scoring ˆβRD = E [βi|˜z], which is the LATE. The availability of the LATE

provides continuous causal evidence of the validity of the slope based scoring policy.

5. In slope based scoring ˆβRD = ˜z, which is the threshold. This provides a consistency

check on the slope based scoring rule.

26

4 Empirical Application: The Targeting of Retargeted

Display Advertising

4.1 Retargeted Display Advertising

In this subsection, we describe the context of our empirical application. We partnered

with a major advertiser conducting a retargeted advertising campaign. The advertiser is

a major provider of cellphone services, and also sells equipment to consumers, including

cellphones, accessories, aside from cellular voice, text and data plans. Consumers who visit

any product page, including for cellphone plans and equipment, but depart the website

without completing a purchase are eligible for a retargeted display advertising campaign.

Retargeted display advertising campaigns reach customers who have, through their prod-

uct page visits, shown that they are interested in the product. Targeting these customers

with display advertising after they leave the advertiser’s website without making a purchase,

might persuade them to return to the advertiser and complete their purchase either online

or oﬄine. Retargeted display advertising often uses dynamic and personalized creatives, the

eﬀects of which have been studied by Lambrecht and Tucker [2013] and Bleier and Eisenbeiss

[2015].

Retargeted display advertising is a controversial practice. The dynamic creatives are

quite visible to consumers and policy makers who may react negatively to such overt track-

ing. There might be multiple mechanisms behind the response to retargeted advertising.

Sahni et al. [2019] show that such the retargeting eﬀect might be driven by a reminder

mechanism and/or a competitive blocking mechanism (Burke and Srull 1988), whereby re-

targeting reduces the likelihood that the consumer subsequently visits a competing seller’s

website and is targeted in turn by their retargeting advertisements.

Industry reports tout the beneﬁts of retargeting and advertisers report spending as much

as 10% of their marketing budgets on retargeting (add cite to Adroll 2014. also cited in

Sahni et al). However a fundamental issue with evaluating retargeting campaigns is that

27

they involve customers who are highly self-selected, as evidenced by their prior interest in

the advertiser’s products. Thus, correlations between advertising and subsequent actions

might reﬂect this selection, rather than the eﬀect of the display retargeting campaign. To

causally evaluate the eﬀects of retargeting, experimental approaches have been proposed

in the literature (Sahni et al. 2019, Johnson et al. 2017b). An issue with the use of these

approaches is that they are potentially costly because they involve signiﬁcant manpower to

execute.

If retargeting does work then there is the opportunity cost of lost sales for the

control group. The ﬁeld studies reported by Johnson et al. [2017b] and Sahni [2015] used

control groups that were as high as 50% of the experimental observations.

Retargeting experiments can be time-consuming. Sahni et al. [2019] report that the retar-

geting ﬁeld experiment executed by them required about 1 year of data collection. They were

able to obtain causal estimates of the impact of retargeting on revisits but not on conver-

sions. Johnson et al. [2017b] report causal estimates of the conversion eﬀects of retargeting.

Low-cost alternatives for continuous evaluation of the eﬀects of retargeting would be a very

useful alternate methodology to ﬁrms. The practical importance of this advertising practice

suggests that additional causal estimates of the impact of retargeted display advertising on

conversions, such as those reported in this study would beneﬁt our overall understanding

of this practice. Further the oﬄine impact of retargeted display advertising has not been

investigated and our study sheds light on oﬄine eﬀects.

4.2 Machine Learning Scoring and Selection

An additional issue with retargeting campaigns is that while they select users who have shown

interest in the advertiser’s products, they also potentially target consumers who are not

interested in the advertiser’s product, as evidenced by their departure from the advertiser’s

website without completing a purchase. Targeting such consumers with advertising might not

just be wasteful, it might even lead to negative eﬀects if the advertising campaigns irritate the

consumer. They also come at the cost of advertising to other, potentially more promising

28

consumers or using the money spent on those consumers in more positive ways. To deal

with this issue, the focal advertiser in our empirical application partnered with a third-party

AI-enabled marketing and advertising services ﬁrm and used a machine learning algorithm

applied on consumer browsing and (when available) transaction data to select consumers for

a retargeting campaign. While others (Sahni et al. 2019, Johnson et al. 2017b) have studied

the retargeting of display advertising, the use of machine learning for audience selection is

novel to this application. If retargeting involves consumers who are highly self-selected, as

evidenced by their prior interest in the advertiser’s products, then the addition of a machine

learning score and selection policy adds another layer of selection further highlighting the

importance of causal estimates.

Consumers were selected using a machine learning methodology that scored them on

purchase propensity. Historical data were used to train a proprietary algorithm (details of

which we are not privy to) to estimate the likelihood that a customer would be a purchaser.

The data for this algorithm included the pages visited by the customer, their dwell times on

various parts of the website and on speciﬁc pages, their signed-in status, and when available,

their past transaction history. These data were then supplied as inputs to the algorithm to

generate a score, which is correlated to their likelihood of making a prior purchase. The ﬁrm

then arrived at a threshold score, based on its business objectives, above which consumers

were eligible for retargeting. Consumers with score below this threshold were not eligible

for the campaign, and no further action was taken in the case of such consumers. Outcomes

including purchases (both online and oﬄine) were tracked for all customers regardless of their

score and whether they were targeted for the advertising campaign or not. The advertising

campaign lasted for up to two weeks after being triggered, and outcomes for all consumers

were tracked for a period of 6 weeks subsequently.

29

4.3 Data Description

We obtained data for a total of about 1.4 million customers who visited the advertiser’s

website during the period for which we have data, visited one or more product pages but

left the website without completing a purchase. All of these customers were assigned scores

through the proprietary machine learning algorithm described in the previous sub-section.

Of these consumers, about 1.1 million had scores below the threshold of 0 and were ineligible

for the retargeting campaign. There were a little over 0.3 million customers with scores

above the threshold and thereby eligible for the retargeting campaign.

Every customer was tracked both before and after the experiment using tracking cookies

placed by the advertising network on consumers’ devices. Since this network has consumers

signed in on to one of their applications or on their web browsers across devices, this tracking

took place across their devices. Thus, a consumer might trigger an advertising campaign

on their laptop computer, be served ads on the computer as well as their mobile devices,

and complete a purchase on a diﬀerent desktop computer. As long as they are signed in on

their diﬀerent devices at some point of time, their cookies across these devices are linked

and the ﬁrm is able to link the triggering event, advertising campaign status and outcomes.

Both past and prior transactions were tracked using this approach. Also, the ﬁrm tracked

oﬄine purchases (i.e. if the customer walked into a brick and mortar store of the advertiser)

when feasible for all customers who had been in signed in state at the time the advertising

campaign was triggered.

For the purpose of this paper, the pertinent data are the machine learning scores and

their purchases before and after the start of the advertising campaign for them. The key

outcome variables we study is whether they make any purchases (conversion), and separately

in the online and oﬄine channels separately. We also examined the total number of items

purchased overall and in the two channels separately, but the results were very similar to

those for the conversion variable, since most consumers who converted only purchased one

item. Hence, we do not separately report estimates for this set of outcome variables.

30

Table (2) reports the summary statistics for these variables for all customers in the

dataset. A few observations about the summary statistics. Conversion, which refers to one or

more instances of purchase after the triggering event of the retargeting camapaign (a product

page view but without completion of a purchase) is a relatively rare event, with about 0.4%

of consumers converting. This is not atypical of retargeting campaigns in general. About

3/4ths of these conversions take place online, although when one looks at past conversions,

it is about equally distributed between online and oﬄine purchases. Given the category -

mobile devices, accessories and cellphone plans - this is again not surprising, given that the

shift from oﬄine to online retail is more recent in the cellphone context than in many other

categories such as books and personal electronics. Another possibility is that retargeting

with digital ads shifts purchases online. Another observation is that the score has a mean

of about 0.15. Since the score is normalized by the standard deviation, this implies that the

threshold, which is at 0, is about 0.15 of a standard deviation below the mean. This is of

relevance given that the bounds estimator for the ATE relies on the diﬀerence between the

mean score and threshold.

The fact that the threshold, which is at 0, is about 0.15 of a standard deviation below

the mean has another important implication. Recall our previous result from equation (15),

that if ˜z = ¯z, then the RDD estimator recovers the ATE. In this application since ˜z (cid:54)= ¯z, we

can be sure that the RDD estimator does not recover the ATE, a useful clariﬁcation that

can be discerned by simply examining the summary statistics of the score and the value of

the threshold.

Table (3) reports the same summary statistics for consumers whose score was below the

zero threshold. These consumers were not eligible for the retargeting campaign after the

triggering event. They had lower conversion than the average consumer in the sample and

lower levels of conversion in the past. The same applied to number of purchase events as

well. Table (4) reports the summary statistics for consumers who had scores above 0, and

hence were eligible to be included in the retargeting campaign. Reading tables (3) and

31

(4) together, it is clear that higher scores are associated with, on average, higher levels of

past purchase and conversion, with the consumers with scores greater than zero having past

conversion incidences at about 5 times the rate of those with scores below zero, and the

number of purchases at almost 4 times. The machine learning score and the corresponding

threshold are indeed selecting consumers who have a high prior propensity to buy. Since

prior purchase propensity is likely to be correlated with future purchase propensity, a case

can be made that the individuals selected for advertising have a higher propensity to buy.

This selection underscores the importance of a causal estimate.

4.4 Empirical Strategy

In this subsection, we use our RDD-based approach to measure treatment eﬀects of the

retargeting campaign. We use local linear regressions to ﬁnd the limiting values of the

outcomes on the two sides of the threshold, choosing bandwidths for the RDD manually, while

assessing sensitivity to the bandwidth choice. Practically, this is achieved using the following

regression conducted on the subset of the data with scores lying within the bandwidth

intervals around the treatment threshold of 0.

yi = θ0 + θ1zi + θ2xi + θ3zixi + νi

(26)

where yi is the outcome of interest, zi is the score, xi is the binary treatment variable and

νi is a random error. It is standard practice in the RDD literature to incorporate zi and the

interaction of zi and xi as regressors. The parameters θ1 and θ3 reﬂect the diﬀerent slopes

of the outcomes with respect to the score on the two sides of the threshold. The treatment

eﬀect we aim to measure is given by θ2.

Our empirical strategy is to measure the treatment eﬀect for three outcomes of interest

- overall conversion (a binary variable indicating whether the customer made one or more

purchases), and conversion in the online and oﬄine channels. In these three instances, we

32

compare the treatment group (with score above the threshold) with observations that had

scores below threshold and which were not eligible for the retargeting campaign.

In ﬁgure (1) we plot conversion rate from the current campaign against the score. Since

conversion is a binary variable, it would not be meaningful or informative to plot it against

score directly. Instead, we divide the data into equally sized intervals based on the score. We

then compute the average conversion for all observations within the interval, and plot this

mean conversion rate against the score. As can be seen from the ﬁgure, the mean conversion

rate is close to zero at all points below the score of zero. There is a discontinuous jump in

the mean conversion rate for all the score intervals above the threshold score of zero. This

discontinuity is what we rely on to ﬁnd the RDD estimates of the eﬀect of retargeting.

We similarly plot the online and oﬄine conversion rates in ﬁgures (2) and (3) respectively.

We see similar disontinuities there as well, though in the case of oﬄine conversion, the

discontinuity is less sharp, in that the conﬁdence intervals of the nonparametric ﬁt lines on

the two sides overlap to a greater degree than for the other variables.

4.5 Results: Local Average Treatment Eﬀects

In this subsection, we report the results of our empirical analysis. Rather than reporting

the full set of regression results for each outcome variable, we report the main parameter of

interest, which is the treatment eﬀect θ2 in equation (26).

Table (5) reports the results of this analysis. The main ﬁndings is that the LATE shows

that retargeted advertising increases total conversions signiﬁcantly. The estimated coeﬃcient

is 0.034 and by a large magnitude (0.034 incremental conversions relative to a baseline of

0.001 from table (3)). The increase in total conversions are largely driven by signiﬁcant

increases in online conversion; there is no signiﬁcant increase in oﬄine conversions as a result

of the retargeting advertising campaign. Thus, we have demonstrated that RDD can be used

to estimate local average treatment eﬀects of retargeting and that these are statistically

and economically signiﬁcant, particularly in terms of the eﬀect on online conversions and

33

purchases.

The RD estimates in Table (5) provide another important insight. Recall from Equation

(24) that if the ﬁrm’s targeting is based on slope based scoring, then ˜βRD = ˜z. In other words

the RDD estimate is equal to the threshold. In our application the threshold is equal to zero

and since the RDD estimate is statistically diﬀerent from zero (and estimated very precisely),

it provides evidence that the ﬁrm is not engaged in slope based scoring. This result combined

with correlation of the score with prior purchases in ﬁgure 4 increases our conﬁdence that the

ﬁrm is most likely engaged in some form of purchase propensity or intercept based scoring.

We note that RDD allows the analyst to reach this conclusion and this ability to diagnose

the nature of the scoring can be very useful when institutional knowledge is not available or

weak.

4.6 Results: Bounds on Average Treatment Eﬀects

Next, we examine if we would be able to obtain upper and lower bounds for the ATE.

Recall from equation (20) that we can obtain these bounds if we knew the variance of the

treatment eﬀects, i.e. var (βi). This is not known to us, but we can see that in the case of the

conversion variables, the treatment eﬀects can be reasonably bounded between 0 and 1, since

the outcome itself is binary. Assume that the treatment eﬀects are uniformly distributed

between 0 and 1. When this happens, it is easy to see that var (βi) = 0.083, which is

obtained from the expression for the variance of a uniform distribution.

With this level of the variance of the treatment eﬀect, we obtain the lower and upper

bounds of the ATE and report it in table (7). Also reported are the lower and upper bounds

when we have smaller variances in the treatment eﬀects given that the uniform distribution

assumption is perhaps conservative. We arbitrarily choose the values for the variance at

0.0005, 0.01, 0.03 and 0.06 and assess what the bounds of the ATE are. Since the expression

for the bounds in equation (20) is linear, the standard errors for the bounds estimates of

ATE are the same as those for the LATE. We ﬁnd that lower bound of ATE in the case

34

of the conservative uniform distribution assumption for treatment eﬀects is not statistically

signiﬁcant but the upper bound is (and always will be if the LATE is statistically signiﬁcant).

When the variance of the treatment eﬀects is lower at 0.005, the lower bound is positive but

statistically insigniﬁcant. For higher variances, the lower bound is typically not statistically

signiﬁcant in this application. However, in other applications, especially where the treatment

eﬀects are stronger, one might see signiﬁcant results for both bounds. Overall the upper

bounds in table 7 suggest that the ATE is economically meaningful. The upper bounds of

11% and 14.2% are similar to the ATE estimates reported by Johnson et al. [2017a].

We replicate this analysis for online conversion and report the results in table (8). We

ﬁnd that in this case, both the lower and upper bounds are signiﬁcant and positive for a

variance of the treatment eﬀect set at 0.005. For higher variances, the lower bound is not

statistically signiﬁcant.

Thus, we can see that we can obtain bounds on the ATE using the RDD design given

an estimate or guess of the variance of the treatment eﬀect. This is a potentially powerful

new result in the context of RDD, which has so far used only for estimation of local average

treatment eﬀects. We also note that the bounds suggest that the ATE can potentially be

zero, but this might simply indicate the degree of response or slope heterogeneity and hence

an opportunity for slope based targeting. Such bounds can be very useful to decision makers

who are interested in the ROI of advertising campaigns. Further given that ﬁrms conduct

many diﬀerent advertising campaigns concurrently which might make it impractical to ex-

perimentally evaluate all campaigns, obtaining bounds in this manner for certain campaigns

can be helpful.

4.7 Robustness checks

In this sub-section, we assess the power of the RD estimator, particularly given the null eﬀects

in some cases, and also assess the sensitivity of RD estimates to the choice of bandwidth,

or the window around the treatment threshold within which observations are used for the

35

estimation procedure. Using a signiﬁcance level of 95% and power of 80%, we compute

the minimum sample size needed to detect the eﬀects we estimate for overall conversion,

online conversion and oﬄine conversion. We ﬁnd that the minimal sample size are 556

observations (across treatment and control) for detecting the lift in conversion we estimate,

549 observations for online conversion, and 5866 observations for oﬄine conversions. Given

that our sample was 4886 observations within the bandwidth, we have adequate power in

the sample for detecting both overall conversion and online conversion. The data are a little

under=powered for detecting oﬄine conversion, but is adequate when the power of the test

is set at 70% instead of 80% (the minimum number of observations in this case is 4612, lower

than the 4886 we have).

There are some placebo tests we conduct to assess robustness of our results. One set

of placebo tests is to ﬁnd RD esimates at the same threshold for a set of variables where

we do not expect to ﬁnd any treatment eﬀect. One such variable is past conversions. We

should not expect any eﬀect of treatment on past conversions. Figures (4) through (6) plot

the prior conversion rates against the score. We see that the conﬁdence intervals for the

non-parametric ﬁt curves on the two sides of the threshold overlap greatly, to the extent

that the conﬁdence interval on the left of the threshold is almost entirely subsumed within

the conﬁdence interval on the right. This is true for all three outcome variables - overall

conversion, online conversion and oﬄine conversion.

Another placebo test we conduct is to compute the RD estimates at other randomly

determined thresholds. We ﬁnd that the treatment eﬀects are insigniﬁcant at 92 out of

100 randomly determined thresholds on either side of the actual threshold. This can be

seen visually in ﬁgures (1) through (3) as well, where there are no other obvious points of

discontinuity like the one at the actual threshold.

Finally, we assess sensitivity of our estimates to the choice of bandwidth and report

these for conversion as an outcome in table (9). As the bandwidth increases, the potential

bias in the RD estimate increases due to the reliance on observations farther away from

36

the threshold. Recall that the RDD-based estimate is based on the limiting values of the

outcomes on the two sides of the threshold, and hence inclusion of more observations farther

from that threshold can increase the bias of the estimates, especially with our use of local

linear regressions on the two sides of the threshold to estimate the limiting values. On

the other hand, reducing the bandwidth reduces our degrees of freedom and leads to the

estimates being insigniﬁcant. Thus, our reported estimates are at a bandwidth of 0.02.

5 Conclusion

In this paper, we examine the use of regression discontinuity designs (RDD) for the esti-

mation of treatment eﬀects in contexts where the marketing treatment is based on scoring

customers based on their past behaviors using a machine learning algorithm and assigning

treatment to customers above a predetermined threshold. Such approaches to targeting are

commonly founds across a variety of domains from advertising to recommendation systems

to customized pricing. We draw attention to RDD as a natural application to ﬁnding local

average treatment eﬀects in these contexts.

We further show that under some conditions, we can use RDD to go beyond local average

treatment eﬀects to ﬁnding average treatment eﬀects or bounds on the ATE when point

estimates are not feasible. We apply these insights to two types of machine learning based

targeting systems - involving intercept-based scores and slope-based scores. For intercept

based scores, the RDD estimates provide a low cost and timely approach to obtain causal

estimates. In the case of slope-based scoring systems, RDD can provide continuous causal

validation of the underlying machine learning algorithm itself.

We present an empirical application in the context of the targeting of retargeted advertis-

ing. Machine learning scores were constructed from customers’ past browsing and transaction

history. In this application, we ﬁnd that there are signiﬁcant eﬀects of retargeted advertising

on conversions in the online channel, but not in the oﬄine channel. We also ﬁnd the bounds

37

on the local average treatment eﬀects and ﬁnd statistically signiﬁcant estimates of both the

lower and upper bounds under certain assumptions on the variance of the treatment eﬀects.

Finally we discuss some limitations of our proposed approach. We need some assumptions

to be true for our approach to work. There should be no other source of discontinuity at

the treatment threshold other than the treatment itself. This rules out relatively simple

algorithms that violate the necessary continuity condition for the score. Our results for

ﬁnding bounds on the average treatment eﬀects require the score to be uncorrelated with

other factors that aﬀect the treatment eﬀect. While this is a reasonable and commonly

employed assumption for many of the contexts we propose our method for, it is not a testable

assumption and may not be reasonable in some contexts. We do not make the case that

our approach is a substitute for experimentation.

It should be viewed as a complement

to other approaches, particularly experiments, for obtaining estimates of the causal eﬀects

of the marketing treatment, and these can be used to periodically validate the continuous

measurement that our approach facilitates.

Overall, we suggest that this is a useful approach to conduct continuous and low cost

evaluation of marketing treatments in a variety of contexts with behavioral targeting based

on underlying machine learning algorithms.

References

G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a
survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734–749, 2005.

R. Agrawal, T. Imieliński, and A. Swami. Mining association rules between sets of items in
large databases. In Proceedings of the 1993 ACM SIGMOD international conference on
Management of data, pages 207–216, 1993.

J. D. Angrist and J.-S. Pischke. The credibility revolution in empirical economics: How better
research design is taking the con out of econometrics. Journal of economic perspectives,
24(2):3–30, 2010.

J. D. Angrist and M. Rokkanen. Wanna get away? regression discontinuity estimation of

38

exam school eﬀects away from the cutoﬀ. Journal of the American Statistical Association,
110(512):1331–1344, 2015.

E. Ascarza, S. A. Neslin, O. Netzer, Z. Anderson, P. S. Fader, S. Gupta, B. G. S. Hardie,
A. Lemmens, B. Libai, D. Neal, F. Provost, and R. Schrift. In pursuit of enhanced customer
retention management: Review, key issues, and future directions. Customer Needs and
Solutions, 5(1-2):65–81, 2018.

S. Athey, G. Imbens, T. Pham, and S. Wager. Estimating average treatment eﬀects: Supple-
mentary analyses and remaining challenges. American Economic Review, 107(5):278–281,
2017.

R. C. Blattberg. Assessing and capturing the soft beneﬁts of scanning," a study conducted

for the coca-cola retailing research council. May, 3:1–43, 1988.

A. Bleier and M. Eisenbeiss. Personalized online advertising eﬀectiveness: The interplay of

what, when, and where. Marketing Science, 34(5):669–688, 2015.

L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.

R. E. Bucklin, J. M. Lattin, A. Ansari, S. Gupta, D. Bell, E. Coupey, J. D. Little, C. Mela,
A. Montgomery, and J. Steckel. Choice and the internet: From clickstream to research
stream. Marketing Letters, 13(3):245–258, 2002.

R. R. Burke and T. K. Srull. Competitive interference and consumer memory for advertising.

Journal of consumer research, 15(1):55–68, 1988.

J. Davidson, B. Liebald, J. Liu, P. Nandy, T. V. Vleet, U. Gargi, S. Gupta, Y. He, M. Lam-
bert, B. Livingstone, and D. Sampath. The youtube recommendation system. 10: Pro-
ceedings of the fourth ACM conference on Recommender systems, pages 293–296, 2010.

J.-P. Dube and S. Misra. Personalized pricing and customer welfare. Working Paper, Uni-

versity of Chicago., 2020.

D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of

peer eﬀects. Working Paper, MIT Sloan School., 2017.

D. Eckles, N. Ignatiadis, S. Wager, and H. Wu. Noise-induced randomization in regression

discontinuity designs. arXiv preprint arXiv:2004.09458, 2020.

J. Fan and I. Gijbels. Local Polynomial Modeling and its Applications. Chapman & Hall,

London, 1996.

C. A. Gomez-Uribe and N. Hunt. The netﬂix recommender system: Algorithms, business
value, and innovation. ACM Transactions on Management Information Systems, 6(4):
13:1–19, 2015.

Google, April 2020. URL https://support.google.com/google-ads/answer/2701222?hl=en.

39

B. R. Gordon, F. Zettelmeyer, N. Bhargava, and D. Chapsky. A comparison of approaches
to advertising measurement: Evidence from big ﬁeld experiments at facebook. Marketing
Science, 38(2):193–225, 2019.

B. R. Gordon, K. Jerath, Z. Katona, S. Narayanan, J. Shin, and K. Wilbur. Ineﬃciencies in

digital advertising markets. Journal of Marketing, 85(1):7–25, 2021.

J. Hahn, P. Todd, and W. van der Klaauw. Identiﬁcation and estimation of treatment eﬀects

with a regression discontinuity design. Econometrica, 69:201–209., 2001.

W. R. Hartmann and D. Klapper. Super bowl ads. Marketing Science, 37(1):78–96, 2018.

X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers,
and J. Q. Candela. Practical lessons from predicting clicks on ads at facebook. In Eighth
International Workshop on Data Mining for Online Advertising, pages 1–9, 2014.

G. Hitsch and S. Misra. Heterogeneous treatment eﬀects and optimal targeting policy eval-

uation. Working Paper, University of Chicago., 2018.

G. W. Imbens and T. Lemieux. Regression discontinuity designs: A guide to practice.

Journal of Econometrics, 142(2):615–635, 2008.

G. A. Johnson, R. A. Lewis, and E. I. Nubbemeyer. Ghost ads: Improving the economics of
measuring online ad eﬀectiveness. Journal of Marketing Research, 54(6):867–884, 2017a.

G. A. Johnson, R. A. Lewis, and E. I. Nubbemeyer. Ghost ads: Improving the economics of
measuring online ad eﬀectiveness. Journal of Marketing Research, 44:867–884, 2017b.

P. Kannan, W. Reinartz, and P. C. Verhoef. The path to purchase and attribution modeling:
Introduction to special section. International Journal of Research in Marketing, 2016.

A. Lambrecht and C. Tucker. When does retargeting work? information speciﬁcity in online

advertising. Journal of Marketing research, 50(5):561–576, 2013.

D. S. Lee and T. Lemeiux. Regression discontinuity designs in economics. Journal of Eco-

nomic Literature, 48(2):281–355, 2010.

J. Liaukonyte, T. Teixeira, and K. C. Wilbur. Television advertising and online shopping.

Marketing Science, 34(3):311–330, 2015.

A. L. Montgomery, S. Li, K. Srinivasan, and J. C. Liechty. Modeling online browsing and

path analysis using clickstream data. Marketing science, 23(4):579–595, 2004.

H. S. Nair, W. R. Hartmann, and S. Narayanan. Identifying causal marketing mix eﬀects

using a regression discontinuity design. Marketing Science, 30(6):1079–1097, 2011.

H. S. Nair, S. Misra, W. J. H. IV, R. Mishra, and A. Acharya. Big data and marketing
analytics in gaming: Combining empirical models and ﬁeld experimentation. Marketing
Science, 36(5):699–725, 2017.

40

S. Narayanan and K. Kalyanam. Position eﬀects in search advertising and their moderators:

A regression discontinuity approach. Marketing Science, 34(3):388–407, 2015.

S. Narayanan and P. Manchanda. Heterogeneous learning and the targeting of marketing

communication for new products. Marketing Science, 28(3):424–441, 2009.

J. Pearl and D. Mackenzie. The Book of Why: The New Science of Cause and Eﬀect. Basic

Books, Inc., 2018.

N. Pujol. Freemium: attributes of an emerging business model. Available at SSRN 1718663,

2010.

P. E. Rossi, R. E. McCulloch, and G. M. Allenby. The value of purchase history data in

target marketing. Marketing Science, 15(4):321–340, 1996.

N. S. Sahni. Eﬀect of temporal spacing between advertising exposures: Evidence from
online ﬁeld experiments. Quantitative Marketing and Economics, 13(3):203–247, 2015.
ISSN 1570-7156.

N. S. Sahni, S. Narayanan, and K. Kalyanam. An experimental investigation of the eﬀects of
retargeted advertising: The role of frequency and timing. Journal of Marketing Research,
56(3):401–418, 2019.

E. B. Seufert. Freemium economics: Leveraging analytics and user segmentation to drive

revenue. Elsevier, 2013.

A. Sharma, J. M. Hofman, and D. J. Watts. Estimating the causal impact of recommendation
systems from observational data. In ACM Conference on Economics and Computation,
2015.

D. Shepard. The new direct marketing: how to implement a proﬁt-driven database marketing-

strategy. Irwin, 1990.

B. Smith and G. Linden. Two decades of recommender systems at amazon.com.

IEEE

Internet Computing, 21(3):12–18, 2017.

R. Sparapani, C. Spanbauer, and R. McCulloch. Nonparametric machine learning and eﬃ-
cient computation with bayesian additive regression trees: the bart r package. Journal of
Statistical Software, pages 1–71, 2019.

N. Syam and A. Sharma. Waiting for a sales renaissance in the fourth industrial revolu-
tion: Machine learning and artiﬁcial intelligence in sales research and practice. Industrial
Marketing Management, 69:135–146, 2018.

S. Wager and S. Athey. Estimation and inference of heterogeneous treatment eﬀects using
random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.

A. A. Zoltners, P. Sinha, and S. E. Lorimer. Sales force eﬀectiveness: A framework for
researchers and practitioners. Journal of Personal Selling & Sales Management, 28(2):
115–131, 2008.

41

Table 1: Summary of Key Theoretical Results

Scoring Method

Required Condition

Intercept

Intercept

Intercept or Slope

Slope

Slope

zi ⊥ βi

˜z = ¯z

None

None

None

Result
ˆβRD = E [βi]
ˆβRD = E [βi]

RDD Estimate Provides

ATE

ATE

E [βi] ⊂ ˆβRD ±

(cid:113) var(βi)

var(zi) |˜z − ¯z|

ˆβRD = E [βi|˜z]
ˆβRD = ˜z

Bounds on ATE

LATE

Consistency Check

Table 2: Summary Statistics

Statistic

N

Mean

St. Dev.

Min

Pctl(25)

Pctl(75) Max

Conversion
Conversion - Online
Conversion - Oﬄine
Purchases
Purchases - Online
Purchases - Oﬄinbe
Prior Conversion
Prior Conversion - Online
Prior Conversion - Oﬄine
Prior Purchases
Prior Purchases - Online
Prior Purchases - Oﬄine
Score

1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913
1,403,913

0.004
0.003
0.001
0.004
0.003
0.001
0.009
0.005
0.004
0.013
0.006
0.007
0.147

0.061
0.052
0.033
0.075
0.062
0.042
0.093
0.068
0.066
0.252
0.130
0.212
1.108

0
0
0
0
0
0
0
0
0
0
0
0
−16.460

0
0
0
0
0
0
0
0
0
0
0
0
−0.180

0
0
0
0
0
0
0
0
0
0
0
0
−0.030

1
1
1
9
9
6
1
1
1
145
39
145
7.490

42

Table 3: Summary Statistics - Customers with Score Lower than Threshold (i.e. Score < 0)

Statistic

N

Mean

St. Dev.

Min

Pctl(25)

Pctl(75)

Max

Conversion
Conversion - Online
Conversion - Oﬄine
Purchases
Purchases - Online
Purchases - Oﬄine
Prior Conversion
Prior Conversion - Online
Prior Conversion - Oﬄine
Prior Purchases
Prior Purchases - Online
Prior Purchases - Oﬄine
Score

0.001
1,099,810
0.0004
1,099,810
0.0002
1,099,810
0.001
1,099,810
0.0005
1,099,810
0.0002
1,099,810
0.005
1,099,810
0.002
1,099,810
0.002
1,099,810
0.008
1,099,810
0.004
1,099,810
0.004
1,099,810
1,099,810 −0.319

0.024
0.021
0.013
0.030
0.026
0.015
0.068
0.049
0.048
0.250
0.123
0.214
0.591

0
0
0
0
0
0
0
0
0
0
0
0
−16.460

0
0
0
0
0
0
0
0
0
0
0
0
−0.250

0
0
0
0
0
0
0
0
0
0
0
0
−0.030

1
1
1
5
5
5
1
1
1
145
39
145
−0.010

Table 4: Summary Statistics - Customers with Score Higher than Threshold (i.e. Score > 0)

Statistic

N

Mean

St. Dev. Min

Pctl(25)

Pctl(75) Max

Conversion
Conversion - Online
Conversion - Oﬄine
Purchases
Purchases - Online
Purchases - Oﬄine
Prior Conversion
Prior Conversion - Online
Prior Conversion - Oﬄine
Prior Purchases
Prior Purchases - Online
Prior Purchases - Oﬄine
Score

304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103
304,103

0.015
0.011
0.004
0.017
0.012
0.005
0.024
0.013
0.012
0.030
0.015
0.016
1.832

0.122
0.104
0.066
0.151
0.123
0.084
0.152
0.111
0.107
0.258
0.152
0.202
0.885

0
0
0
0
0
0
0
0
0
0
0
0
0.010

0
0
0
0
0
0
0
0
0
0
0
0
1.180

0
0
0
0
0
0
0
0
0
0
0
0
2.480

1
1
1
9
9
6
1
1
1
27
27
25
7.490

43

Table 5: Treatment Eﬀects - Regression Discontinuity Estimates

Coeﬃcient

Std. Err.

p-value

N

Conversion
Conversion - Online
Conversion - Oﬄine

0.034
0.032
0.001

0.009
0.007
0.005

0.0001
0.00001
0.774

4, 886
4, 886
4, 886

Table 6: Treatment Eﬀects - Correlational Estimates

Coeﬃcient

Std. Err.

p-value

N

Conversion
Conversion - Online
Conversion - Oﬄine

0.014
0.010
0.004

0.0001
0.0000
0.0000

0.0000
0.0000
0.0000

1, 403, 913
1, 403, 913
1, 403, 913

Table 7: Bounds for ATE - Conversion

coeﬃcients

stderr

pvalues

deg.freedom

LATE
ATE - Lower Bound [βi ∼ U (0, 1)]
ATE - Upper Bound [βi ∼ U (0, 1)]
ATE - Lower Bound [var(βi) = 0.005]
ATE - Upper Bound [var(βi) = 0.005]
ATE - Lower Bound [var(βi) = 0.01]
ATE - Upper Bound [var(βi) = 0.01]
ATE - Lower Bound [var(βi) = 0.03]
ATE - Upper Bound [var(βi) = 0.03]
ATE - Lower Bound [var(βi) = 0.06]
ATE - Upper Bound [var(βi) = 0.06]

0.034
−0.095
0.163
0.002
0.065
−0.044
0.111
−0.044
0.111
−0.076
0.143

0.009
0.009
0.009
0.009
0.009
0.009
0.009
0.009
0.009
0.009
0.009

0.0001
0
0
0.406
0
0.00000
0
0.00000
0
0
0

4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886

44

Table 8: Bounds for ATE - Online Conversion

coeﬃcients

stderr

pvalues

deg.freedom

LATE
ATE - Lower Bound [βi ∼ U (0, 1)]
ATE - Upper Bound [βi ∼ U (0, 1)]
ATE - Lower Bound [var(βi) = 0.005]
ATE - Upper Bound [var(βi) = 0.005]
ATE - Lower Bound [var(βi) = 0.01]
ATE - Upper Bound [var(βi) = 0.01]
ATE - Lower Bound [var(βi) = 0.03]
ATE - Upper Bound [var(βi) = 0.03]
ATE - Lower Bound [var(βi) = 0.06]
ATE - Upper Bound [var(βi) = 0.06]

0.032
−0.097
0.161
0.001
0.064
−0.012
0.077
−0.045
0.110
−0.077
0.142

0.007
0.007
0.007
0.007
0.007
0.007
0.007
0.007
0.007
0.007
0.007

0.00001
0
0
0.463
0
0.043
0
0
0
0
0

4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886
4, 886

Table 9: Sensitivity of Treatment Eﬀect Estimates to Bandwidth Choice - Conversion

Coeﬃcient

Std.Err.

p-value

N

Bandwidth = 0.02
Bandwidth = 0.03
Bandwidth = 0.01

0.034
0.072
0.026

0.009
0.016
0.007

0.0001
0.00001
0.0001

4, 886
5, 195
4, 362

45

Figure 1: Conversion vs. Score

Figure 2: Online Conversion vs. Score

46

Figure 3: Oﬄine Conversion vs. Score

Figure 4: Prior Conversion vs. Score

47

Figure 5: Prior Online Conversion vs. Score

Figure 6: Prior Oﬄine Conversion vs. Score

48

