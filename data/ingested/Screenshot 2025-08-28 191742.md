# Screenshot 2025-08-28 191742.png

Our goal here will be to continuously monitor the flood of new LLMs, techniques, and trends, especially those relevant to finance, and classify them in a useful way. However, the challenge here is that LLM landscape
evolves rapidly as new models are released almost daily, and each of these models have unique capabilities. A static approach will not be enough and we would need an automated pipeline to gather and organize this
information.

Here are the possible techniques we could use. Firrst we have Automated Feed and Database: We could set up a pipeline to scrape or query sources of LLM announcements (eg scientific papers, model hubs like
Hugging Face, Al news sites). For example, one project would systematically scrape an LLM repository daily, extracting each model's name, family, downloads, tags, and description or it could collect model metadata
(for example model size, domain, release date, performance metrics) and store it in a database.) The pros of these is that it will be straightfoward to implement as we can use tools like APIs or web scrapers to collect
raw info about new models like their descriptions and how much they're used and store them in a database which later would make it easy for the Advisor module to find and filter models based on different criteria.
However the potential cons for that is that we would need to keep updating our scrapers or API setups because sources might often change how their data is structured. Also, these tools don’t understand the data—
they just grab it so even after collecting everything, we might still need to organize or label the data ourself to actually make use of it.
For the next part we have LLM/NLP-Based Classification: which we can use to classify new models and techniques. For instance, when a new model/paper is detected, use an existing LLM or a fine-tuned classifier
to analyze the description and label it (e.g., Domain: finance vs. general; Technique: new architecture, fine-tune method, etc.). For this we can use a smaller model (like a BERT-based classifier) or even prompting a
service like GPT-4 can do the job. This will automate understanding of each model and helps turn raw data into searchable categories of interest. Here we will have to train a custom classifier which requires some
labeled data so we could either start by manually labeling a few examples of model descriptions to train or prompt the system. Here we also have to be carefull of possible errors or irrelevant output at this point as
well.

Optional we could also have a Trend Analytics (i.e Popularity Tracking) which would track which models are trending at the moment. We might want to consider this as it will add a quantitative layer to our tracker —
you can quickly see which new LLMs are gaining traction in the community. That can be something we can look at. (this can be done by monitoring metrics like downloads, GitHub stars, or discussion frequency. For
example, one approach tracked the popularity trajectory of open-source models by recording download counts over time and visualizing trends. Implementing this might involve time-series databases or dashboards
(as in the example, which evolved from simple CSV logs to a cloud database and Streamlit dashboard. This is valuable for recommending “tried-and-true” models versus obscure ones. Trending metrics are
automatically collected, reducing manual effort in identifying hot models. Cons: Depends on available data; you need access to metrics (Hugging Face APIs for downloads, for instance). Popularity doesn’t always
equal suitability - a model could be trending generally but not be the best for finance-specific tasks. Thus, we'd use this in combination with content-based classification. Also, setting up dashboards or cloud storage
adds complexity (though there are templates and tools available).
