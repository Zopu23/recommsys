# Screenshot 2025-08-28 191849.png

Explainer Module: Our goal for this modules is make LLM concepts, outputs, or decisions understandable to non-technical users in finance. This module acts as an explainer or interpreter, translating jargon and
complex ideas into plain English. For every model the Advisor recommends, the Explainer should also break down why that model was chosen. Also what | wanted to point out at this point is that there is a lof of
discussion on hallucianation, biases and regulations of LLMS so | think here we could also think about how address these issues for our users. For example whenever a model is recommended by our advisor, we could
include a section where we talk about the known limitations of this specific model or even the advantages ( for example we could say stuff like “this model tends to halucciane when X happens" or “This model shows
bias towards certain genders.” Also here we could futher guide our user on the usage of the model recommended like suggesting retrieval-augmented generation (RAG) setups to reduce hallucinations, or prompt
engineering advice to help users get more reliable results. Basically, we should not only recommend a model but also guide users on how to use it responsibly and effectively.

SOME of the Challenges here is that Many Al terms (like fine-tuning, embeddings, overfitting, hallucination) are unfamiliar to non-tech users. and Also, explaining why a model was recommended or how an LLM

arrived at an answer is hard as we get into Al explainabii
Possible Techniques: One straightforward approach is G

ity.
lossary-Base Retrieval where we could maintain a curated glossary of Al/LLM terms and their plain-language definitions. This could be as simple as a

dictionary. So When the user encounters an unknown term or requests an explanation, the system looks it up in this glossary and returns the explanation. The pros here is that we get a lot of Accuracy and consistency
here and is computationally cheap, however this method is very limited and its not quite as good as the others.

Next is Explanation (Prompting): where we would use an LLM (maybe the same one behind Advisor, or a smaller one) to generate explanations on demand. This technique could handle not just definitions but also
explanations of a model's behavior, strengths and weaknesses, common failure cases, and even guidance on how to best interact with it using prompts.lt would give users quick, Al-generated insights into why a

model was recommended and how to get the best resu

ts from it. This method is Flexible and comprehensive as the LLM would be able to answer even something that is not predefined. However there is always the

risk that the LLM might output an explanation that is too technical or even incorrect.

And lastly we have Fine-Tuned Explainer Model where we could fine-tune a smaller open-source model to be an “explainer.” We can take an instruction-following model (like a smaller LLaMA-2 or GPT-J variant) and
fine tune it on this data to produce a reliable model that outputs easy-to-understand answers. This is a good idea as it can be optimized for our domain (for example, using finance-specific language or analogies
when it makes sense.) This approach aligns with our objectives as it makes sure explanations truly help users and over time this model can be updated as new terminology emerges. However, this obviosly requires
more development effort and data. We would need to prepare a training dataset (which could be a bottleneck — however, we might bootstrap it from existing glossaries or even synthetic data generated by GPT-4 and

then verified by experts). A practical strategy is to mix tl
But that needs to be decided.

hese approaches for reliability. We could start with a predefined glossary for the most common terms and then, integrate an LLM for more in-depth questions.
